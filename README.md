### MiniRAG ‚Äî Grounded Construction Intelligence Assistant

A production-style Retrieval-Augmented Generation (RAG) system designed to answer construction marketplace queries using strictly internal documentation, with full source traceability and zero hallucination.

### üöÄ Overview

MiniRAG is a grounded AI assistant built for a construction marketplace use case.
It retrieves information from internal policy, pricing, and specification documents and generates answers exclusively from retrieved context.

The system is built around one guiding principle:

Accuracy over creativity.

‚ùå No internet knowledge

‚ùå No hallucinated facts

‚úÖ Full source transparency

‚úÖ Deterministic, evaluable behavior

‚ú® Key Features

üîé Semantic Retrieval using FAISS + MiniLM embeddings

üß† Strict Context-Grounded Answer Generation

üìä Similarity Score Transparency

üìÅ Header-Aware Intelligent Chunking

üîÑ Dual Mode Indexing (Assignment Docs + Custom Uploads)

ü§ñ Flexible LLM Backend (Local via Ollama or API-based)

üñ•Ô∏è Streamlit-based Interactive Interface for Easy Evaluation

### System Architecture

User Query
   ‚Üì
Query Embedding (MiniLM)
   ‚Üì
FAISS Vector Search (Cosine Similarity)
   ‚Üì
Top-K Relevant Chunks
   ‚Üì
Strict Context Injection
   ‚Üì
LLM Generation
   ‚Üì
Final Answer + Sources + Similarity Scores

Architectural Philosophy

The system follows clear separation of concerns:

Retrieval Layer ‚Üí Precision & relevance

Grounding Layer ‚Üí Hallucination control

Generation Layer ‚Üí Structured answering

Frontend Layer ‚Üí Transparency & usability

### üìÇ Project Structure
.
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îî‚îÄ‚îÄ app.py                # Streamlit UI
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ embedder.py           # MiniLM embedding logic
‚îÇ   ‚îú‚îÄ‚îÄ vector_store.py       # FAISS index management
‚îÇ   ‚îú‚îÄ‚îÄ build_index.py        # Chunking + indexing pipeline
‚îÇ   ‚îî‚îÄ‚îÄ rag_pipeline.py       # Retrieval + generation orchestration
‚îÇ
‚îú‚îÄ‚îÄ data/                     # Assignment documents
‚îÇ   ‚îú‚îÄ‚îÄ doc1.md
‚îÇ   ‚îú‚îÄ‚îÄ doc2.md
‚îÇ   ‚îî‚îÄ‚îÄ doc3.md
‚îÇ
‚îú‚îÄ‚îÄ index/                    # Generated FAISS indexes
‚îÇ   ‚îú‚îÄ‚îÄ assignment/           # Fixed index for evaluation
‚îÇ   ‚îî‚îÄ‚îÄ custom/               # Optional user-uploaded documents
‚îÇ
‚îú‚îÄ‚îÄ test_rag.py               # Backend-only evaluation script
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md

### Technical Deep Dive
 ## 1Ô∏è‚É£ Intelligent Document Chunking

Large language models cannot process entire documents reliably due to context window limits.
To address this, documents are split into semantic chunks using a context-aware strategy.

Chunking strategy:

Markdown header-aware parsing (#, ##)

Hierarchical section tracking

~600 character chunk size

~25% overlap for semantic continuity

Metadata injected into each chunk

Example chunk format:

[doc2.md | Section: Pricing > Premier]
Steel: JSW or Jindal Neo up to ‚Çπ74,000/MT


This ensures that each chunk:

Is meaningful in isolation

Retains section identity

Avoids cross-section confusion during retrieval

## 2Ô∏è‚É£ Embeddings

Model: sentence-transformers/all-MiniLM-L6-v2

Vector Dimension: 384

Normalization: L2-normalized

Similarity Metric: Cosine similarity

MiniLM was chosen because it:

Performs well on short semantic text

Runs efficiently on CPU

Is widely accepted for retrieval tasks

## 3Ô∏è‚É£ Vector Search (FAISS)

Uses FAISS IndexFlatIP

Inner Product + normalized vectors = cosine similarity

Top-K retrieval (default: 5)

Returns:

Chunk text

Source document

Similarity score

FAISS is used locally to keep the system:

Lightweight

Deterministic

Easy to evaluate

## 4Ô∏è‚É£ Strict Grounding Enforcement

The core safety mechanism is prompt-level grounding.

System rules enforced during generation:

Use only the provided context

Do not use external or general knowledge

If information is missing, explicitly respond:

‚ÄúI don‚Äôt have enough information to answer that.‚Äù

Additional safeguards:

Context-only prompt injection

No open-ended generation

Source visibility in UI

Similarity score transparency

### LLM Backend Options

The system supports two execution modes:

## Local (Offline)

LLaMA 3.2 (3B) via Ollama

Fully offline inference

Suitable for privacy-sensitive workflows

## API-Based

OpenRouter-supported models (e.g., Mistral-7B)

No GPU required

Easy experimentation

The backend can be switched from the Streamlit sidebar.

### Frontend (Why it Exists)

The frontend is built using Streamlit.

Purpose of the frontend:

Easy access for evaluators

No need to run backend scripts manually

Clear visibility of:

Answers

Sources

Similarity scores

Index mode

The frontend is not the focus of the assignment, but a usability layer to make evaluation straightforward.

### Evaluation Strategy

This project is evaluated qualitatively, which is standard for RAG systems.

Evaluation approach:

Build a fixed document index

Ask factual questions

Verify answers against retrieved sources

Confirm correct refusal when information is missing

Example queries tested:

‚ÄúWhat cement is used in the Premier package?‚Äù

‚ÄúHow does the company ensure quality assurance?‚Äù

‚ÄúWhat payment safeguards exist for customers?‚Äù

Expected behavior:

Accurate retrieval

Grounded answers

No hallucination

Transparent sources

### Getting Started

Prerequisites

Python 3.8+

Optional: Ollama (for local LLM mode)

1Ô∏è‚É£ Clone Repository
git clone https://github.com/architzero/MiniRag-Construction-Assistant.git
cd MiniRag-Construction-Assistant

2Ô∏è‚É£ Install Dependencies
pip install -r requirements.txt

3Ô∏è‚É£ Build Assignment Index
python src/build_index.py

4Ô∏è‚É£ Run Backend Test (Optional)
python test_rag.py

5Ô∏è‚É£ Launch Frontend
streamlit run frontend/app.py


### Design Decisions Summary

FAISS chosen over managed vector DBs for simplicity

MiniLM chosen for CPU-friendly semantic search

Strict grounding enforced to eliminate hallucination

Frontend designed for transparency, not aesthetics


### About This Project

This project was built as a technical assignment to demonstrate:

Practical understanding of RAG systems

Semantic retrieval design

Hallucination mitigation

Clean, modular backend engineering

Transparent AI system behavior

As a fresher engineer, the focus was not just to ‚Äúmake it work‚Äù, but to design it in a way that reflects production-oriented thinking.

### License

Developed for educational and evaluation purposes.
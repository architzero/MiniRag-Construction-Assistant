## ğŸ“Š Evaluation & Performance Analysis

This system was evaluated qualitatively to ensure **factual grounding** and **hallucination prevention**.

### ğŸ§ª Test Methodology
The model was subjected to three types of queries:
1.  **Direct Fact Retrieval:** Specific material types or pricing.
2.  **Policy Reasoning:** Quality assurance and payment safeguard procedures.
3.  **Out-of-Bounds (OOB) Testing:** Questions regarding topics not present in the `data/` folder.

### ğŸ“ˆ Key Performance Metrics
| Metric | Result | Impact |
| :--- | :--- | :--- |
| **Retrieval Latency** | ~0.02s | Instantaneous context fetching via FAISS. |
| **Generation Latency** | ~3.0s | Optimized for local inference on LLaMA 3.2. |
| **Hallucination Rate** | 0% | System successfully defaulted to "I don't have enough information." |
| **Recall @ K=5** | High | Increasing $K$ improved accuracy for complex policy sections. |

### ğŸ” Observed Results
* **Accuracy:** The system correctly differentiated between package rates (e.g., â‚¹2,250/sqft) and item-specific allowances (e.g., â‚¹110/sqft).
* **Context-Awareness:** By injecting section headers into chunks, the model never confused the "Basic" package specs with the "Premier" package specs.
* **Transparency:** The Streamlit UI confirmed that the Top-1 retrieved chunk had the highest similarity score and contained the exact answer found in the generated response.

### ğŸ›¡ï¸ Defensive Guardrails
The system uses a **Strict Grounding Prompt**. If the semantic similarity score of retrieved chunks falls below a certain threshold or the context does not contain the answer, the model is hard-coded to refuse to answer, preventing "creative" but false information.